{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib nbagg\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import dynet\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/en.pos.train'\n",
    "sentences = open(data_path, 'r').read().strip().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, tags = defaultdict(int), set()\n",
    "for sentence in sentences:\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split('\\t')\n",
    "        word_count[word] += 1\n",
    "        tags.add(tag)\n",
    "tags = list(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in word_count.keys() if word_count[word]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['<UNK>', '<s>', '</s>'] + words\n",
    "feat_tags = ['<s>'] + tags\n",
    "output_tags = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {word: i for i, word in enumerate(words)}\n",
    "feat_tags_dict = {tag: i for i, tag in enumerate(feat_tags)}\n",
    "output_tag_dict = {tag: i for i, tag in enumerate(output_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagid2tag_str(id):\n",
    "    return output_tags[id]\n",
    "\n",
    "def tag2id(tag):\n",
    "    return output_tag_dict[tag]\n",
    "\n",
    "def feat_tag2id(tag):\n",
    "    return feat_tags_dict[tag]\n",
    "\n",
    "def word2id(word):\n",
    "    return word_dict[word] if word in word_dict else word_dict['<UNK>']\n",
    "\n",
    "def num_words():\n",
    "    return len(words)\n",
    "\n",
    "def num_tag_feats():\n",
    "    return len(feat_tags)\n",
    "\n",
    "def num_tags():\n",
    "    return len(output_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = open(data_path, 'r').read().strip().split('\\n\\n')\n",
    "writer = open(data_path+'.data', 'w')\n",
    "\n",
    "for sen in sens:\n",
    "    lines = sen.strip().split('\\n')\n",
    "    ws, ts = ['<s>', '<s>'], ['<s>', '<s>']\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split()\n",
    "        ws.append(word)\n",
    "        ts.append(tag)\n",
    "    ws += ['</s>', '</s>']\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        feats = [ws[i], ws[i + 1], ws[i + 2], ws[i + 3], ws[i + 4], ts[i], ts[i + 1]]\n",
    "        label = ts[i + 2]\n",
    "        writer.write('\\t'.join(feats) + '\\t' + label + '\\n')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise weights in hyperbolic space\n",
    "\n",
    "word_embed_dim, pos_embed_dim = 5,5\n",
    "\n",
    "word_embedding = torch.normal(torch.zeros(len(words),word_embed_dim))\n",
    "tag_embedding=torch.normal(torch.zeros(len(feat_tags),pos_embed_dim))\n",
    "radii_we=torch.rand(len(words))\n",
    "radii_te=torch.rand(len(feat_tags))\n",
    "we_norm=torch.sum(word_embedding**2,dim=1)\n",
    "te_norm=torch.sum(tag_embedding**2,dim=1)\n",
    "correction_we=radii_we/torch.sqrt(we_norm)\n",
    "correction_te=radii_te/torch.sqrt(te_norm)\n",
    "corr_tile_we=correction_we.repeat(word_embed_dim,1)\n",
    "pre_word_embedding=corr_tile_we.transpose(0,1)*word_embedding\n",
    "hyp_word_embedding=pre_word_embedding.clone().detach().requires_grad_(True)\n",
    "corr_tile_te=correction_te.repeat(pos_embed_dim,1)\n",
    "pre_tag_embedding=corr_tile_te.transpose(0,1)*tag_embedding\n",
    "hyp_tag_embedding=pre_tag_embedding.clone().detach().requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5 * word_embed_dim + 2 * pos_embed_dim\n",
    "\n",
    "hidden_dim, minibatch_size = 200, 1000\n",
    "\"\"\"\n",
    "hidden_layer_we=torch.normal(torch.zeros(hidden_dim,input_dim))\n",
    "radii_hwe=torch.rand(hidden_dim)\n",
    "hwe_norm=torch.sum(hidden_layer_we**2,dim=1)\n",
    "correction_hwe=radii_hwe/torch.sqrt(hwe_norm)\n",
    "corr_tile_hwe=correction_hwe.repeat(input_dim,1)\n",
    "hidden_we=corr_tile_hwe.transpose(0,1)*hidden_layer_we\n",
    "hidden_layer=hidden_we.clone().detach().requires_grad_(True)\n",
    "\n",
    "\"\"\"\n",
    "hidden_layers_we={}\n",
    "hidden_layers_te={}\n",
    "\n",
    "# initialize the hidden layers in the hyperbolic space. Notice that we are in a subspace of it because we are not normalising each istance\n",
    "for i in range(5):\n",
    "    hidden_layer_we=torch.normal(torch.zeros(hidden_dim,word_embed_dim))\n",
    "    radii_hwe=torch.rand(hidden_dim)\n",
    "    hwe_norm=torch.sum(hidden_layer_we**2,dim=1)\n",
    "    correction_hwe=radii_hwe/torch.sqrt(hwe_norm)\n",
    "    corr_tile_hwe=correction_hwe.repeat(word_embed_dim,1)\n",
    "    hidden_we=corr_tile_hwe.transpose(0,1)*hidden_layer_we\n",
    "    hidden_layers_we[i]=hidden_we.clone().detach().requires_grad_(True)\n",
    "    \n",
    "\n",
    "for i in range(2):\n",
    "    hidden_layer_te=torch.normal(torch.zeros(hidden_dim,pos_embed_dim))\n",
    "    radii_hte=torch.rand(hidden_dim)\n",
    "    hte_norm=torch.sum(hidden_layer_te**2,dim=1)\n",
    "    correction_hte=radii_hte/torch.sqrt(hte_norm)\n",
    "    corr_tile_hte=correction_hte.repeat(pos_embed_dim,1)\n",
    "    hidden=corr_tile_hte.transpose(0,1)*hidden_layer_te\n",
    "    hidden_layers_te[i]=hidden.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "# define the hidden layer bias term and initialize it as constant 0.2.\n",
    "hidden_layer_biases = 0.2*torch.ones((7,hidden_dim))\n",
    "hidden_layer_bias=hidden_layer_biases.clone().detach().requires_grad_(True)\n",
    "\n",
    "# define the output weight.\n",
    "output_layers = torch.normal(torch.zeros(num_tags(), hidden_dim)).clone().detach().requires_grad_(True)\n",
    "\n",
    "# define the bias vector and initialize it as zero.\n",
    "\n",
    "output_bias = torch.zeros(num_tags(),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arccosh(x):\n",
    "    c0 = torch.log(x)\n",
    "    c1 = torch.log1p(torch.sqrt(x * x - 1) / x)\n",
    "    return c0 + c1\n",
    "\n",
    "def hyp_dist(u,v):\n",
    "    de=2*(torch.sum((u-v)**2,dim=1))/((1-torch.sum(u**2,dim=1))*(1-torch.sum(v**2,dim=1)))\n",
    "    dist=arccosh(1+de)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "seed = 1008\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features):\n",
    "    \n",
    "   # extract word and tags ids\n",
    "    word_ids = [word2id(word_feat) for word_feat in features[0:5]]\n",
    "    tag_ids = [feat_tag2id(tag_feat) for tag_feat in features[5:]]\n",
    "    hidden_out=torch.zeros(hidden_dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    # consider outputs for each position and sum them all together\n",
    "    hyp_con=torch.cat((hyp_word_embedding[word_ids[0]],hyp_word_embedding[word_ids[1]],hyp_word_embedding[word_ids[2]],hyp_word_embedding[word_ids[3]],hyp_word_embedding[word_ids[4]],hyp_tag_embedding[tag_ids[0]],hyp_tag_embedding[tag_ids[1]]),0)\n",
    "    hyp_conc=hyp_con/(torch.sum(hyp_con**2)+1e-2)\n",
    "    c=hyp_conc.repeat(hidden_dim,1)\n",
    "    de=1+2*(torch.sum((hidden_layer-c)**2,dim=1))/((1-torch.sum(hidden_layer**2,dim=1))*(1-torch.sum(c**2,dim=1)))\n",
    "    print(de)\n",
    "    hyp_distance=torch.log(de)+torch.log1p(torch.sqrt(de**2-1)/de)\n",
    "    print(hyp_distance)\n",
    "    \n",
    "    hidden_out=F.relu(1-hyp_distance)\n",
    "    print(hidden_out)\n",
    "    output=output_layers@hidden_out+output_bias\n",
    "    print(output)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    for i,wid in enumerate(word_ids):\n",
    "        c=hyp_word_embedding[wid].repeat(hidden_dim,1)\n",
    "        #hidden_out+=1-hyp_dist(hidden_layers_we[i],c)\n",
    "        hidden_out+=F.relu(1-hyp_dist(hidden_layers_we[i],c)+hidden_layer_bias[i]) #question: do we want to fire if all the points are relatively close to the hidden layer?\n",
    "        \n",
    "    for j,tag in enumerate(tag_ids):\n",
    "        d=hyp_tag_embedding[tag].repeat(hidden_dim,1)\n",
    "        #hidden_out+=1-hyp_dist(hidden_layers_te[j],d)\n",
    "        hidden_out+=F.relu(1-hyp_dist(hidden_layers_te[j],d)+hidden_layer_bias[j+5])\n",
    "    output = output_layers@F.relu(hidden_out) + output_bias\n",
    "    \n",
    "    \n",
    "    # return a list of outputs\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ws):\n",
    "   # first putting two start symbols\n",
    "    ws = ['<s>', '<s>'] + ws + ['</s>', '</s>']\n",
    "    ts = ['<s>', '<s>']\n",
    "\n",
    "    for i in range(2, len(ws) - 2):\n",
    "        features = ws[i - 2:i + 3] + ts[i - 2:i]\n",
    "\n",
    "       # running forward\n",
    "        output = forward(features)\n",
    "        \n",
    "\n",
    "       # getting best tag\n",
    "        best_tag_id = torch.argmax(output)\n",
    "\n",
    "       # assigning the best tag\n",
    "        ts.append(tagid2tag_str(best_tag_id))\n",
    "\n",
    "    return ts[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(data_path+'.data', 'r').read().strip().split('\\n')\n",
    "optimizer=torch.optim.SGD([output_bias,output_layers,hidden_layer_bias],lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(train_data,hidden_layers_we,hidden_layers_te,hyp_word_embedding,hyp_tag_embedding):\n",
    "        losses = [] # minibatch loss vector\n",
    "        random.shuffle(train_data) # shuffle the training data.\n",
    "\n",
    "        for line in train_data:\n",
    "            fields = line.strip().split('\\t')\n",
    "            features, label, gold_label = fields[:-1], fields[-1], tag2id(fields[-1])\n",
    "            result = forward(features)\n",
    "\n",
    "            # getting loss with respect to negative log softmax function and the gold label; and appending to the minibatch losses.\n",
    "            loss_func=torch.nn.LogSoftmax(dim=0)\n",
    "            softmax=-loss_func(result)\n",
    "            loss = softmax[gold_label]\n",
    "            losses.append(loss)\n",
    "\n",
    "            if len(losses) >= minibatch_size:\n",
    "                minibatch_loss_value= sum(losses) / len(losses) \n",
    "\n",
    "                # printing info and plotting\n",
    "                                                    \n",
    "                minibatch_loss_value.backward(retain_graph=True) # calling pytorch to run backpropagation\n",
    "                optimizer.step() # calling pytorch to change parameter values with respect to current backpropagation\n",
    "                \n",
    "                for i in range(5): \n",
    "                    norm_sq=torch.sum(hidden_layers_we[i].data**2,dim=1)\n",
    "                    var=(1e-2*((1-norm_sq.repeat(word_embed_dim,1).transpose(0,1))**2)/4)*hidden_layers_we[i].grad.data\n",
    "                    hidden_layers_we[i].data=hidden_layers_we[i].data-var\n",
    "                    if torch.max(torch.sum(hidden_layers_we[i].data**2,dim=1))>1:\n",
    "                        hidden_layers_we[i].data=hidden_layers_we[i].data/(torch.sum(hidden_layers_we[i].data**2,dim=1)+1e-5)\n",
    "                    hidden_layers_we[i].grad.data.zero_()\n",
    "                for i in range(2):\n",
    "                    norm_sq=torch.sum(hidden_layers_te[i].data**2,dim=1)\n",
    "                    var=1e-2*(((1-norm_sq.repeat(pos_embed_dim,1).transpose(0,1))**2)/4)*hidden_layers_te[i].grad.data\n",
    "                    hidden_layers_te[i].data=hidden_layers_te[i].data-var\n",
    "                    if torch.max(torch.sum(hidden_layers_te[i].data**2,dim=1))>1:\n",
    "                        hidden_layers_te[i].data=hidden_layers_te[i].data/(torch.sum(hidden_layers_te[i].data**2,dim=1)+1e-5)\n",
    "                    hidden_layers_we[i].grad.data.zero_()\n",
    "                \"\"\"\n",
    "                hl_norm_sq=torch.sum(hidden_layer.data**2,dim=1)\n",
    "                var=(1e-2*((1-hl_norm_sq.repeat(input_dim,1).transpose(0,1))**2)/4)*hidden_layer.grad.data\n",
    "                hidden_layer.data=hidden_layer.data-var\n",
    "                if torch.max(torch.sum(hidden_layer.data**2,dim=1))>1:\n",
    "                    hidden_layer.data=hidden_layer.data/(torch.sum(hidden_layer.data**2,dim=1)+1e-5)\n",
    "                hidden_layer.grad.data.zero_()\n",
    "                \"\"\"    \n",
    "                hwe_norm_sq=torch.sum(hyp_word_embedding.data**2,dim=1)\n",
    "                hwe_var=1e-2*(((1-hwe_norm_sq.repeat(word_embed_dim,1).transpose(0,1))**2)/4)*hyp_word_embedding.grad.data\n",
    "                hyp_word_embedding.data=hyp_word_embedding.data-hwe_var\n",
    "                if torch.max(torch.sum(hyp_word_embedding.data**2,dim=1))>1:\n",
    "                    hyp_word_embedding.data=hyp_word_embedding.data/(torch.sum(hyp_word_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_word_embedding.grad.data.zero_()\n",
    "                hte_norm_sq=torch.sum(hyp_tag_embedding.data**2,dim=1)\n",
    "                hte_var=1e-2*(((1-hte_norm_sq.repeat(pos_embed_dim,1).transpose(0,1))**2)/4)*hyp_tag_embedding.grad.data\n",
    "                hyp_tag_embedding.data=hyp_tag_embedding.data-hte_var\n",
    "                if torch.max(torch.sum(hyp_tag_embedding.data**2,dim=1))>1:\n",
    "                    hyp_tag_embedding.data=hyp_tag_embedding.data/(torch.sum(hyp_tag_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_tag_embedding.grad.data.zero_()\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "                # empty the loss vector and refresh the memory of dynetnet\n",
    "                losses = []\n",
    "                optimizer.zero_grad()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    model.populate(filename)\n",
    "\n",
    "def save(filename):\n",
    "    model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print('epoch',epoch+1)\n",
    "    train_iter(train_data,hidden_layers_we,hidden_layers_te,hyp_word_embedding,hyp_tag_embedding)\n",
    "    \n",
    "print('finished training!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'data/en.pos.dev.raw'\n",
    "writer = open(test_file+'.output.hyperbballbias.dim5', 'w')#change dimension here\n",
    "for sentence in open(test_file, 'r'):\n",
    "    words = sentence.strip().split()\n",
    "    tags = decode(words)\n",
    "    output = [word + '\\t' + tag for word, tag in zip(words, tags)]\n",
    "    writer.write('\\n'.join(output) + '\\n\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(w_test_file,data_file):\n",
    "    true=0\n",
    "    compare=open(data_file,'r')\n",
    "    l=[]\n",
    "    k=[]\n",
    "    for sentence1 in compare:\n",
    "        words1=sentence1.strip().split()\n",
    "        if len(words1)==2:\n",
    "            l.append(words1[1])\n",
    "    for sentence2 in open(w_test_file,'r'):\n",
    "        words2=sentence2.strip().split()\n",
    "        if len(words2)==2:\n",
    "            k.append(words2[1])\n",
    "    for i in range(len(l)):\n",
    "        if l[i]==k[i]:\n",
    "            true+=1\n",
    "    accuracy=true/len(l)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_test('data/en.pos.dev.raw.output.hyperbball.dim20','data/en.pos.dev'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
