{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib nbagg\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import dynet\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/en.pos.train'\n",
    "sentences = open(data_path, 'r').read().strip().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, tags = defaultdict(int), set()\n",
    "for sentence in sentences:\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split('\\t')\n",
    "        word_count[word] += 1\n",
    "        tags.add(tag)\n",
    "tags = list(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in word_count.keys() if word_count[word]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['<UNK>', '<s>', '</s>'] + words\n",
    "feat_tags = ['<s>'] + tags\n",
    "output_tags = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {word: i for i, word in enumerate(words)}\n",
    "feat_tags_dict = {tag: i for i, tag in enumerate(feat_tags)}\n",
    "output_tag_dict = {tag: i for i, tag in enumerate(output_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagid2tag_str(id):\n",
    "    return output_tags[id]\n",
    "\n",
    "def tag2id(tag):\n",
    "    return output_tag_dict[tag]\n",
    "\n",
    "def feat_tag2id(tag):\n",
    "    return feat_tags_dict[tag]\n",
    "\n",
    "def word2id(word):\n",
    "    return word_dict[word] if word in word_dict else word_dict['<UNK>']\n",
    "\n",
    "def num_words():\n",
    "    return len(words)\n",
    "\n",
    "def num_tag_feats():\n",
    "    return len(feat_tags)\n",
    "\n",
    "def num_tags():\n",
    "    return len(output_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = open(data_path, 'r').read().strip().split('\\n\\n')\n",
    "writer = open(data_path+'.data', 'w')\n",
    "\n",
    "for sen in sens:\n",
    "    lines = sen.strip().split('\\n')\n",
    "    ws, ts = ['<s>', '<s>'], ['<s>', '<s>']\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split()\n",
    "        ws.append(word)\n",
    "        ts.append(tag)\n",
    "    ws += ['</s>', '</s>']\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        feats = [ws[i], ws[i + 1], ws[i + 2], ws[i + 3], ws[i + 4], ts[i], ts[i + 1]]\n",
    "        label = ts[i + 2]\n",
    "        writer.write('\\t'.join(feats) + '\\t' + label + '\\n')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_dim, pos_embed_dim = 100, 100\n",
    "\n",
    "word_embedding = torch.normal(torch.zeros(len(words),word_embed_dim))\n",
    "tag_embedding=torch.normal(torch.zeros(len(feat_tags), pos_embed_dim))\n",
    "radii_we=torch.rand(len(words))\n",
    "radii_te=torch.rand(len(feat_tags))\n",
    "we_norm=torch.sum(word_embedding**2,dim=1)\n",
    "te_norm=torch.sum(tag_embedding**2,dim=1)\n",
    "correction_we=radii_we/torch.sqrt(we_norm)\n",
    "correction_te=radii_te/torch.sqrt(te_norm)\n",
    "corr_tile_we=correction_we.repeat(word_embed_dim,1)\n",
    "pre_word_embedding=corr_tile_we.transpose(0,1)*word_embedding\n",
    "hyp_word_embedding=pre_word_embedding.clone().detach().requires_grad_(True)\n",
    "\n",
    "corr_tile_te=correction_te.repeat(pos_embed_dim,1)\n",
    "pre_tag_embedding=corr_tile_te.transpose(0,1)*tag_embedding\n",
    "hyp_tag_embedding=pre_tag_embedding.clone().detach().requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arccosh(x):\n",
    "    c0 = torch.log(x)\n",
    "    c1 = torch.log1p(torch.sqrt(x * x - 1) / x)\n",
    "    return c0 + c1\n",
    "\n",
    "def hyp_dist(u,v):\n",
    "    de=2*(torch.sum((u-v)**2,dim=1))/((1-torch.sum(u**2,dim=1))*(1-torch.sum(v**2,dim=1)))\n",
    "    dist=arccosh(1+de)\n",
    "        \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features):\n",
    "    \n",
    "   # extract word and tags ids\n",
    "    word_ids = [word2id(word_feat) for word_feat in features[0:5]]\n",
    "    tag_ids = [feat_tag2id(tag_feat) for tag_feat in features[5:]]\n",
    "    \n",
    "    \"\"\"\n",
    "    # consider outputs for each position and sum them all together\n",
    "    hyp_con=torch.cat((hyp_word_embedding[word_ids[0]],hyp_word_embedding[word_ids[1]],hyp_word_embedding[word_ids[2]],hyp_word_embedding[word_ids[3]],hyp_word_embedding[word_ids[4]],hyp_tag_embedding[tag_ids[0]],hyp_tag_embedding[tag_ids[1]]),0)\n",
    "    hyp_conc=hyp_con/(torch.sum(hyp_con**2)+1e-2)\n",
    "    c=hyp_conc.repeat(hidden_dim,1)\n",
    "    de=1+2*(torch.sum((hidden_layer-c)**2,dim=1))/((1-torch.sum(hidden_layer**2,dim=1))*(1-torch.sum(c**2,dim=1)))\n",
    "    print(de)\n",
    "    hyp_distance=torch.log(de)+torch.log1p(torch.sqrt(de**2-1)/de)\n",
    "    print(hyp_distance)\n",
    "    \n",
    "    hidden_out=F.relu(1-hyp_distance)\n",
    "    print(hidden_out)\n",
    "    output=output_layers@hidden_out+output_bias\n",
    "    print(output)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    c1=hyp_tag_embedding[tag_ids[0]].repeat(len(feat_tags),1)\n",
    "    c2=hyp_word_embedding[word_ids[2]].repeat(len(feat_tags),1)\n",
    "    dist1=hyp_dist(c1,hyp_tag_embedding)\n",
    "    dist2=hyp_dist(c2,hyp_tag_embedding)\n",
    "    loss_func=torch.nn.LogSoftmax(dim=0)\n",
    "    soft1=loss_func(-dist1)\n",
    "    soft2=loss_func(-dist2)\n",
    "    output=-(soft1+soft2)\n",
    "    \n",
    "    \n",
    "    # return a list of outputs\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ws):\n",
    "   # first putting two start symbols\n",
    "    ws = ['<s>', '<s>'] + ws + ['</s>', '</s>']\n",
    "    ts = ['<s>', '<s>']\n",
    "\n",
    "    for i in range(2, len(ws) - 2):\n",
    "        features = ws[i - 2:i + 3] + ts[i - 2:i]\n",
    "\n",
    "       # running forward\n",
    "        dist1,dist2,dist3 = forward(features)\n",
    "        softmax=torch.nn.Softmax()\n",
    "        soft1=softmax(-dist1)\n",
    "        soft2=softmax(-dist2)\n",
    "        output=soft1*soft2\n",
    "        \n",
    "\n",
    "       # getting best tag\n",
    "        best_tag_id = torch.argmax(output)\n",
    "\n",
    "       # assigning the best tag\n",
    "        ts.append(tagid2tag_str(best_tag_id))\n",
    "\n",
    "    return ts[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280551\n"
     ]
    }
   ],
   "source": [
    "train_data_start = open(data_path+'.data', 'r').read().strip().split('\\n')\n",
    "print(len(train_data_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data_start[:8000]\n",
    "minibatch_size=1000\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(train_data,hyp_word_embedding,hyp_tag_embedding):\n",
    "        losses = [] # minibatch loss vector\n",
    "        random.shuffle(train_data) # shuffle the training data.\n",
    "        loss_values=[]\n",
    "\n",
    "        for line in train_data:\n",
    "            fields = line.strip().split('\\t')\n",
    "            with torch.autograd.detect_anomaly():\n",
    "                features, label, gold_label = fields[:-1], fields[-1], tag2id(fields[-1])\n",
    "                output = forward(features)\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # getting loss with respect to negative log softmax function and the gold label; and appending to the minibatch losses.\n",
    "                loss = output[gold_label]\n",
    "                loss.backward()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if len(losses) >= minibatch_size:\n",
    "                minibatch_loss_value= sum(losses) / len(losses) \n",
    "                print(minibatch_loss_value)\n",
    "                \n",
    "\n",
    "\n",
    "                # printing info and plotting\n",
    "                loss_values.append(minibatch_loss_value)\n",
    "\n",
    "                                                    \n",
    "                minibatch_loss_value.backward(retain_graph=True) # calling pytorch to run backpropagation\n",
    "                \n",
    "                    \n",
    "                hwe_norm_sq=torch.sum(hyp_word_embedding.data**2,dim=1)\n",
    "                hwe_var=1e-2*(((1-hwe_norm_sq.repeat(word_embed_dim,1).transpose(0,1))**2)/4)*hyp_word_embedding.grad.data\n",
    "                hyp_word_embedding.data=hyp_word_embedding.data-hwe_var\n",
    "                if torch.max(torch.sum(hyp_word_embedding.data**2,dim=1))>1:\n",
    "                    hyp_word_embedding.data=hyp_word_embedding.data/(torch.sum(hyp_word_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_word_embedding.grad.data.zero_()\n",
    "                hte_norm_sq=torch.sum(hyp_tag_embedding.data**2,dim=1)\n",
    "                hte_var=1e-2*(((1-hte_norm_sq.repeat(pos_embed_dim,1).transpose(0,1))**2)/4)*hyp_tag_embedding.grad.data\n",
    "                hyp_tag_embedding.data=hyp_tag_embedding.data-hte_var\n",
    "                if torch.max(torch.sum(hyp_tag_embedding.data**2,dim=1))>1:\n",
    "                    hyp_tag_embedding.data=hyp_tag_embedding.data/(torch.sum(hyp_tag_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_tag_embedding.grad.data.zero_()\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "                # empty the loss vector and refresh the memory of dynetnet\n",
    "                losses = []\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    model.populate(filename)\n",
    "\n",
    "def save(filename):\n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: RuntimeWarning: Traceback of forward call that caused the error:\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n",
      "    self.run()\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"/Users/francescopreta/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-278-84dfb8618e37>\", line 3, in <module>\n",
      "    train_iter(train_data,hyp_word_embedding,hyp_tag_embedding)\n",
      "  File \"<ipython-input-276-a99f4222efd5>\", line 10, in train_iter\n",
      "    output = forward(features)\n",
      "  File \"<ipython-input-272-b82da19609e8>\", line 26, in forward\n",
      "    dist1=hyp_dist(c1,hyp_tag_embedding)\n",
      "  File \"<ipython-input-264-6f3dec5b2367>\", line 7, in hyp_dist\n",
      "    de=2*(torch.sum((u-v)**2,dim=1))/((1-torch.sum(u**2,dim=1))*(1-torch.sum(v**2,dim=1)))\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'DivBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-84dfb8618e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyp_word_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyp_tag_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished training!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-a99f4222efd5>\u001b[0m in \u001b[0;36mtrain_iter\u001b[0;34m(train_data, hyp_word_embedding, hyp_tag_embedding)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# getting loss with respect to negative log softmax function and the gold label; and appending to the minibatch losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgold_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'DivBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('epoch',i+1) \n",
    "    train_iter(train_data,hyp_word_embedding,hyp_tag_embedding)\n",
    "    \n",
    "print('finished training!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-545d6e0f0794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a87b9cfb98db>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(ws)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0;31m# getting best tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbest_tag_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m        \u001b[0;31m# assigning the best tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(input, dim, keepdim)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \"\"\"\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'contiguous'"
     ]
    }
   ],
   "source": [
    "test_file = 'data/en.pos.dev.raw'\n",
    "writer = open(test_file+'.output', 'w')\n",
    "for sentence in open(test_file, 'r'):\n",
    "    words = sentence.strip().split()\n",
    "    tags = decode(words)\n",
    "    output = [word + '\\t' + tag for word, tag in zip(words, tags)]\n",
    "    writer.write('\\n'.join(output) + '\\n\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
