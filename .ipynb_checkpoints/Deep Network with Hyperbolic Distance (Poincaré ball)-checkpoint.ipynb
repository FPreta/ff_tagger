{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger with Hyperbolic Distance-based pairings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This POS tagger uses the architecture of the tagger in Deep Network with Euclidean Distance, but substitutes embedding layers for words, parts of speech and neurons with hyperbolic embeddings, using the hyperbolic ball model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib nbagg\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import dynet\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "seed = 1008\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vocabulary from training data. See Pytorch Example or Dynet Example for a more explicit explanation of this part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/en.pos.train'\n",
    "sentences = open(data_path, 'r').read().strip().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, tags = defaultdict(int), set()\n",
    "for sentence in sentences:\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split('\\t')\n",
    "        word_count[word] += 1\n",
    "        tags.add(tag)\n",
    "tags = list(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in word_count.keys() if word_count[word]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['<UNK>', '<s>', '</s>'] + words\n",
    "feat_tags = ['<s>'] + tags\n",
    "output_tags = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {word: i for i, word in enumerate(words)}\n",
    "feat_tags_dict = {tag: i for i, tag in enumerate(feat_tags)}\n",
    "output_tag_dict = {tag: i for i, tag in enumerate(output_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagid2tag_str(id):\n",
    "    return output_tags[id]\n",
    "\n",
    "def tag2id(tag):\n",
    "    return output_tag_dict[tag]\n",
    "\n",
    "def feat_tag2id(tag):\n",
    "    return feat_tags_dict[tag]\n",
    "\n",
    "def word2id(word):\n",
    "    return word_dict[word] if word in word_dict else word_dict['<UNK>']\n",
    "\n",
    "def num_words():\n",
    "    return len(words)\n",
    "\n",
    "def num_tag_feats():\n",
    "    return len(feat_tags)\n",
    "\n",
    "def num_tags():\n",
    "    return len(output_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = open(data_path, 'r').read().strip().split('\\n\\n')\n",
    "writer = open(data_path+'.data', 'w')\n",
    "\n",
    "for sen in sens:\n",
    "    lines = sen.strip().split('\\n')\n",
    "    ws, ts = ['<s>', '<s>'], ['<s>', '<s>']\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split()\n",
    "        ws.append(word)\n",
    "        ts.append(tag)\n",
    "    ws += ['</s>', '</s>']\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        feats = [ws[i], ws[i + 1], ws[i + 2], ws[i + 3], ws[i + 4], ts[i], ts[i + 1]]\n",
    "        label = ts[i + 2]\n",
    "        writer.write('\\t'.join(feats) + '\\t' + label + '\\n')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize our features in a ball of radius 1 of dimension word_embed_dim and pos_embed_dim. In order to do so, we normalize a 2D-array of features drawn from independent normal distributions by their norm in the embedding dimension and multiply each one of them by a different radius in (0,1) drawn from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_dim, pos_embed_dim = 5,5\n",
    "\n",
    "word_embedding = torch.normal(torch.zeros(len(words),word_embed_dim))\n",
    "tag_embedding=torch.normal(torch.zeros(len(feat_tags),pos_embed_dim))\n",
    "radii_we=torch.rand(len(words))\n",
    "radii_te=torch.rand(len(feat_tags))\n",
    "we_norm=torch.sum(word_embedding**2,dim=1)\n",
    "te_norm=torch.sum(tag_embedding**2,dim=1)\n",
    "correction_we=radii_we/torch.sqrt(we_norm)\n",
    "correction_te=radii_te/torch.sqrt(te_norm)\n",
    "corr_tile_we=correction_we.repeat(word_embed_dim,1)\n",
    "pre_word_embedding=corr_tile_we.transpose(0,1)*word_embedding\n",
    "hyp_word_embedding=pre_word_embedding.clone().detach().requires_grad_(True) #adding the last piece for backpropagation\n",
    "corr_tile_te=correction_te.repeat(pos_embed_dim,1)\n",
    "pre_tag_embedding=corr_tile_te.transpose(0,1)*tag_embedding\n",
    "hyp_tag_embedding=pre_tag_embedding.clone().detach().requires_grad_(True) #adding the last piece for backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a similar method to initialise our neurons in hyperbolic space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim, minibatch_size = 200, 1000\n",
    "hidden_layers_we={}\n",
    "hidden_layers_te={}\n",
    "\n",
    "# initialize the hidden layers in the hyperbolic space\n",
    "for i in range(5):\n",
    "    hidden_layer_we=torch.normal(torch.zeros(hidden_dim,word_embed_dim))\n",
    "    radii_hwe=torch.rand(hidden_dim)\n",
    "    hwe_norm=torch.sum(hidden_layer_we**2,dim=1)\n",
    "    correction_hwe=radii_hwe/torch.sqrt(hwe_norm)\n",
    "    corr_tile_hwe=correction_hwe.repeat(word_embed_dim,1)\n",
    "    hidden_we=corr_tile_hwe.transpose(0,1)*hidden_layer_we\n",
    "    hidden_layers_we[i]=hidden_we.clone().detach().requires_grad_(True)\n",
    "    \n",
    "\n",
    "for i in range(2):\n",
    "    hidden_layer_te=torch.normal(torch.zeros(hidden_dim,pos_embed_dim))\n",
    "    radii_hte=torch.rand(hidden_dim)\n",
    "    hte_norm=torch.sum(hidden_layer_te**2,dim=1)\n",
    "    correction_hte=radii_hte/torch.sqrt(hte_norm)\n",
    "    corr_tile_hte=correction_hte.repeat(pos_embed_dim,1)\n",
    "    hidden=corr_tile_hte.transpose(0,1)*hidden_layer_te\n",
    "    hidden_layers_te[i]=hidden.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "# define the hidden layer bias term and initialize it as constant 0.2.\n",
    "hidden_layer_biases = 0.2*torch.ones((7,hidden_dim))\n",
    "hidden_layer_bias=hidden_layer_biases.clone().detach().requires_grad_(True)\n",
    "\n",
    "# define the output weight.\n",
    "output_layers = torch.normal(torch.zeros(num_tags(), hidden_dim)).clone().detach().requires_grad_(True)\n",
    "\n",
    "# define the output bias vector and initialize it as zero.\n",
    "output_bias = torch.zeros(num_tags(),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arccosh(x):\n",
    "    c0 = torch.log(x)\n",
    "    c1 = torch.log1p(torch.sqrt(x * x - 1) / x)\n",
    "    return c0 + c1\n",
    "\n",
    "def hyp_dist(u,v):\n",
    "    de=2*(torch.sum((u-v)**2,dim=1))/((1-torch.sum(u**2,dim=1))*(1-torch.sum(v**2,dim=1)))\n",
    "    dist=arccosh(1+de)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network has to be defined manually, since the backpropagation implemented in standard pytorch models don't include Riemannian Gradient Descent for non-euclidean spaces. The architecture is the following:\n",
    "    1. Embedding layer for words and parts of speech in hyperbolic space.\n",
    "    2. Hidden layer measuring the correlation between each feature and a corresponding set of neurons, with a possible bias. We apply a ReLU, so that only neurons with high correlation will fire, in a similar way to the linear case;\n",
    "    3. Output linear layer applied to a ReLU on the sum of the outputs coming from the linear layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features):\n",
    "    \n",
    "   # extract word and tags ids\n",
    "    word_ids = [word2id(word_feat) for word_feat in features[0:5]]\n",
    "    tag_ids = [feat_tag2id(tag_feat) for tag_feat in features[5:]]\n",
    "    hidden_out=torch.zeros(hidden_dim)\n",
    "    \n",
    "    for i,wid in enumerate(word_ids):\n",
    "        c=hyp_word_embedding[wid].repeat(hidden_dim,1)\n",
    "        hidden_out+=F.relu(1-hyp_dist(hidden_layers_we[i],c)+hidden_layer_bias[i]) \n",
    "        \n",
    "    for j,tag in enumerate(tag_ids):\n",
    "        d=hyp_tag_embedding[tag].repeat(hidden_dim,1)\n",
    "        hidden_out+=F.relu(1-hyp_dist(hidden_layers_te[j],d)+hidden_layer_bias[j+5])\n",
    "    output = output_layers@F.relu(hidden_out) + output_bias\n",
    "      \n",
    "    # return a list of outputs\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ws):\n",
    "   # first putting two start symbols\n",
    "    ws = ['<s>', '<s>'] + ws + ['</s>', '</s>']\n",
    "    ts = ['<s>', '<s>']\n",
    "\n",
    "    for i in range(2, len(ws) - 2):\n",
    "        features = ws[i - 2:i + 3] + ts[i - 2:i]\n",
    "\n",
    "       # running forward\n",
    "        output = forward(features)\n",
    "        \n",
    "\n",
    "       # getting best tag\n",
    "        best_tag_id = torch.argmax(output)\n",
    "\n",
    "       # assigning the best tag\n",
    "        ts.append(tagid2tag_str(best_tag_id))\n",
    "\n",
    "    return ts[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading training data and defining the training function. Note that backpropagation is now split between a standard gradient descent for the outer layers and a Riemannian gradient descent for weights and features defined on a hyperbolic manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(data_path+'.data', 'r').read().strip().split('\\n')\n",
    "optimizer=torch.optim.SGD([output_bias,output_layers,hidden_layer_bias],lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(train_data,hidden_layers_we,hidden_layers_te,hyp_word_embedding,hyp_tag_embedding):\n",
    "        losses = [] # minibatch loss vector\n",
    "        random.shuffle(train_data) # shuffle the training data.\n",
    "\n",
    "        for line in train_data:\n",
    "            fields = line.strip().split('\\t')\n",
    "            features, label, gold_label = fields[:-1], fields[-1], tag2id(fields[-1])\n",
    "            result = forward(features)\n",
    "\n",
    "            # getting loss with respect to negative log softmax function and the gold label; and appending to the minibatch losses.\n",
    "            loss_func=torch.nn.LogSoftmax(dim=0)\n",
    "            softmax=-loss_func(result)\n",
    "            loss = softmax[gold_label]\n",
    "            losses.append(loss)\n",
    "\n",
    "            if len(losses) >= minibatch_size:\n",
    "                minibatch_loss_value= sum(losses) / len(losses) \n",
    "                                                    \n",
    "                minibatch_loss_value.backward(retain_graph=True) # calling pytorch to run backpropagation\n",
    "                optimizer.step() # calling pytorch to change parameter values with respect to current backpropagation\n",
    "                \n",
    "                for i in range(5): \n",
    "                    norm_sq=torch.sum(hidden_layers_we[i].data**2,dim=1)\n",
    "                    var=(1e-2*((1-norm_sq.repeat(word_embed_dim,1).transpose(0,1))**2)/4)*hidden_layers_we[i].grad.data\n",
    "                    hidden_layers_we[i].data=hidden_layers_we[i].data-var\n",
    "                    if torch.max(torch.sum(hidden_layers_we[i].data**2,dim=1))>1:\n",
    "                        hidden_layers_we[i].data=hidden_layers_we[i].data/(torch.sum(hidden_layers_we[i].data**2,dim=1)+1e-5)\n",
    "                    hidden_layers_we[i].grad.data.zero_()\n",
    "                for i in range(2):\n",
    "                    norm_sq=torch.sum(hidden_layers_te[i].data**2,dim=1)\n",
    "                    var=1e-2*(((1-norm_sq.repeat(pos_embed_dim,1).transpose(0,1))**2)/4)*hidden_layers_te[i].grad.data\n",
    "                    hidden_layers_te[i].data=hidden_layers_te[i].data-var\n",
    "                    if torch.max(torch.sum(hidden_layers_te[i].data**2,dim=1))>1:\n",
    "                        hidden_layers_te[i].data=hidden_layers_te[i].data/(torch.sum(hidden_layers_te[i].data**2,dim=1)+1e-5)\n",
    "                    hidden_layers_we[i].grad.data.zero_()\n",
    "                    \n",
    "                hwe_norm_sq=torch.sum(hyp_word_embedding.data**2,dim=1)\n",
    "                hwe_var=1e-2*(((1-hwe_norm_sq.repeat(word_embed_dim,1).transpose(0,1))**2)/4)*hyp_word_embedding.grad.data\n",
    "                hyp_word_embedding.data=hyp_word_embedding.data-hwe_var\n",
    "                if torch.max(torch.sum(hyp_word_embedding.data**2,dim=1))>1:\n",
    "                    hyp_word_embedding.data=hyp_word_embedding.data/(torch.sum(hyp_word_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_word_embedding.grad.data.zero_()\n",
    "                hte_norm_sq=torch.sum(hyp_tag_embedding.data**2,dim=1)\n",
    "                hte_var=1e-2*(((1-hte_norm_sq.repeat(pos_embed_dim,1).transpose(0,1))**2)/4)*hyp_tag_embedding.grad.data\n",
    "                hyp_tag_embedding.data=hyp_tag_embedding.data-hte_var\n",
    "                if torch.max(torch.sum(hyp_tag_embedding.data**2,dim=1))>1:\n",
    "                    hyp_tag_embedding.data=hyp_tag_embedding.data/(torch.sum(hyp_tag_embedding.data**2,dim=1)+1e-5)\n",
    "                hyp_tag_embedding.grad.data.zero_()\n",
    "                \n",
    "                losses = []\n",
    "                optimizer.zero_grad()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print('epoch',epoch+1)\n",
    "    train_iter(train_data,hidden_layers_we,hidden_layers_te,hyp_word_embedding,hyp_tag_embedding)\n",
    "    \n",
    "print('finished training!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to classify our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'data/en.pos.dev.raw'\n",
    "writer = open(test_file+'.output.hyperbballbias.dim5', 'w')#change dimension here\n",
    "for sentence in open(test_file, 'r'):\n",
    "    words = sentence.strip().split()\n",
    "    tags = decode(words)\n",
    "    output = [word + '\\t' + tag for word, tag in zip(words, tags)]\n",
    "    writer.write('\\n'.join(output) + '\\n\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(w_test_file,data_file):\n",
    "    true=0\n",
    "    compare=open(data_file,'r')\n",
    "    l=[]\n",
    "    k=[]\n",
    "    for sentence1 in compare:\n",
    "        words1=sentence1.strip().split()\n",
    "        if len(words1)==2:\n",
    "            l.append(words1[1])\n",
    "    for sentence2 in open(w_test_file,'r'):\n",
    "        words2=sentence2.strip().split()\n",
    "        if len(words2)==2:\n",
    "            k.append(words2[1])\n",
    "    for i in range(len(l)):\n",
    "        if l[i]==k[i]:\n",
    "            true+=1\n",
    "    accuracy=true/len(l)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44657111922632264\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_test('data/en.pos.dev.raw.output.hyperbballbias.dim5','data/en.pos.dev'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
