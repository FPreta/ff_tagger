{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A POS tagger based on deep neural network, Pytorch Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary from the training data\n",
    "First we should read from data file. The data looks like the following\n",
    "```txt\n",
    "Two     NUM\n",
    "of      ADP\n",
    "them    PRON\n",
    "were    AUX\n",
    "being   AUX\n",
    "run     VERB\n",
    "by      ADP\n",
    "2       NUM\n",
    "officials       NOUN\n",
    "of      ADP\n",
    "the     DET\n",
    "Ministry        PROPN\n",
    "of      ADP\n",
    "the     DET\n",
    "Interior        PROPN\n",
    "!       PUNCT\n",
    "\n",
    "The     DET\n",
    "MoI     PROPN\n",
    "in      ADP\n",
    "Iraq    PROPN\n",
    "is      AUX\n",
    "equivalent      ADJ\n",
    "to      ADP\n",
    "the     DET\n",
    "US      PROPN\n",
    "FBI     PROPN\n",
    ",       PUNCT\n",
    "so      ADV\n",
    "this    PRON\n",
    "would   AUX\n",
    "be      VERB\n",
    "like    SCONJ\n",
    "having  VERB\n",
    "J.      PROPN\n",
    "Edgar   PROPN\n",
    "Hoover  PROPN\n",
    "unwittingly     ADV\n",
    "employ  VERB\n",
    "at      ADP\n",
    "a       DET\n",
    "high    ADJ\n",
    "level   NOUN\n",
    "members NOUN\n",
    "of      ADP\n",
    "the     DET\n",
    "Weathermen      PROPN\n",
    "bombers NOUN\n",
    "back    ADV\n",
    "in      ADP\n",
    "the     DET\n",
    "1960s   NOUN\n",
    ".       PUNCT\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/en.pos.train'\n",
    "sentences = open(data_path, 'r').read().strip().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should count the frequency of words and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_count, tags = defaultdict(int), set()\n",
    "for sentence in sentences:\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split('\\t')\n",
    "        word_count[word] += 1\n",
    "        tags.add(tag)\n",
    "tags = list(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assume that words with frequency less than one should be disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in word_count.keys() if word_count[word]>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also take into account special symbols start of a sentence and end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['<UNK>', '<s>', '</s>'] + words\n",
    "feat_tags = ['<s>'] + tags\n",
    "output_tags = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also create string to integer mapping (because neural network libraries work with integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {word: i for i, word in enumerate(words)}\n",
    "feat_tags_dict = {tag: i for i, tag in enumerate(feat_tags)}\n",
    "output_tag_dict = {tag: i for i, tag in enumerate(output_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some auxiliary functions to access the words, tag feature and tag output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagid2tag_str(id):\n",
    "    return output_tags[id]\n",
    "\n",
    "def tag2id(tag):\n",
    "    return output_tag_dict[tag]\n",
    "\n",
    "def feat_tag2id(tag):\n",
    "    return feat_tags_dict[tag]\n",
    "\n",
    "def word2id(word):\n",
    "    return word_dict[word] if word in word_dict else word_dict['<UNK>']\n",
    "\n",
    "def num_words():\n",
    "    return len(words)\n",
    "\n",
    "def num_tag_feats():\n",
    "    return len(feat_tags)\n",
    "\n",
    "def num_tags():\n",
    "    return len(output_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting training data to a csv-style format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = open(data_path, 'r').read().strip().split('\\n\\n')\n",
    "writer = open(data_path+'.data', 'w')\n",
    "\n",
    "for sen in sens:\n",
    "    lines = sen.strip().split('\\n')\n",
    "    ws, ts = ['<s>', '<s>'], ['<s>', '<s>']\n",
    "    for line in lines:\n",
    "        word, tag = line.strip().split()\n",
    "        ws.append(word)\n",
    "        ts.append(tag)\n",
    "    ws += ['</s>', '</s>']\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        feats = [ws[i], ws[i + 1], ws[i + 2], ws[i + 3], ws[i + 4], ts[i], ts[i + 1]]\n",
    "        label = ts[i + 2]\n",
    "        writer.write('\\t'.join(feats) + '\\t' + label + '\\n')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output data should look like the following (__first 5 are word features, the other two are pos features, the last is the pos label__)\n",
    "```txt\n",
    "<s>     <s>     Al      -       Zaman   <s>     <s>     PROPN\n",
    "<s>     Al      -       Zaman   :       <s>     PROPN   PUNCT\n",
    "Al      -       Zaman   :       American        PROPN   PUNCT   PROPN\n",
    "-       Zaman   :       American        forces  PUNCT   PROPN   PUNCT\n",
    "Zaman   :       American        forces  killed  PROPN   PUNCT   ADJ\n",
    ":       American        forces  killed  Shaikh  PUNCT   ADJ     NOUN\n",
    "American        forces  killed  Shaikh  Abdullah        ADJ     NOUN    VERB\n",
    "forces  killed  Shaikh  Abdullah        al      NOUN    VERB    PROPN\n",
    "killed  Shaikh  Abdullah        al      -       VERB    PROPN   PROPN\n",
    "Shaikh  Abdullah        al      -       Ani     PROPN   PROPN   PROPN\n",
    "Abdullah        al      -       Ani     ,       PROPN   PROPN   PUNCT\n",
    "al      -       Ani     ,       the     PROPN   PUNCT   PROPN\n",
    "-       Ani     ,       the     preacher        PUNCT   PROPN   PUNCT\n",
    "Ani     ,       the     preacher        at      PROPN   PUNCT   DET\n",
    ",       the     preacher        at      the     PUNCT   DET     NOUN\n",
    "the     preacher        at      the     mosque  DET     NOUN    ADP\n",
    "preacher        at      the     mosque  in      NOUN    ADP     DET\n",
    "at      the     mosque  in      the     ADP     DET     NOUN\n",
    "the     mosque  in      the     town    DET     NOUN    ADP\n",
    "mosque  in      the     town    of      NOUN    ADP     DET\n",
    "in      the     town    of      Qaim    ADP     DET     NOUN\n",
    "the     town    of      Qaim    ,       DET     NOUN    ADP\n",
    "town    of      Qaim    ,       near    NOUN    ADP     PROPN\n",
    "of      Qaim    ,       near    the     ADP     PROPN   PUNCT\n",
    "Qaim    ,       near    the     Syrian  PROPN   PUNCT   ADP\n",
    ",       near    the     Syrian  border  PUNCT   ADP     DET\n",
    "near    the     Syrian  border  .       ADP     DET     ADJ\n",
    "the     Syrian  border  .       </s>    DET     ADJ     NOUN\n",
    "Syrian  border  .       </s>    </s>    ADJ     NOUN    PUNCT\n",
    "<s>     <s>     [       This    killing <s>     <s>     PUNCT\n",
    "<s>     [       This    killing of      <s>     PUNCT   DET\n",
    "[       This    killing of      a       PUNCT   DET     NOUN\n",
    "This    killing of      a       respected       DET     NOUN    ADP\n",
    "killing of      a       respected       cleric  NOUN    ADP     DET\n",
    "of      a       respected       cleric  will    ADP     DET     ADJ\n",
    "a       respected       cleric  will    be      DET     ADJ     NOUN\n",
    "respected       cleric  will    be      causing ADJ     NOUN    AUX\n",
    "cleric  will    be      causing us      NOUN    AUX     AUX\n",
    "will    be      causing us      trouble AUX     AUX     VERB\n",
    "be      causing us      trouble for     AUX     VERB    PRON\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network in Pytorch\n",
    "Here we introduce the major change from the forked library, which comes from writing the network in Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "%matplotlib nbagg\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "seed = 1008\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has the following architecture (the same of the Dynet example):\n",
    "    1. Embedding layer for words and parts of speech (of respective dimensions word_embed_dim and pos_embed_dim);\n",
    "    2. Hidden layer (linear+ReLu) having as input the concatenation of 5 words and 2 parts of speech corresponding to the first two words and output of dimension hidden_dim;\n",
    "    3. A linear output layer trying to predict the part of speech corresponding to the third word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_dim, pos_embed_dim = 2,2\n",
    "input_dim=5*word_embed_dim+2*pos_embed_dim\n",
    "hidden_dim,output_dim=200,len(feat_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_tagging(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(POS_tagging, self).__init__()\n",
    "        self.word_embeddings=nn.Embedding(len(words),word_embed_dim)\n",
    "        self.tag_embeddings=nn.Embedding(len(feat_tags),pos_embed_dim)\n",
    "        self.network=torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim),nn.ReLU(),nn.Linear(hidden_dim,output_dim))\n",
    "       \n",
    "    def forward(self, features):\n",
    "        word_ids = torch.tensor([word2id(word_feat) for word_feat in features[0:5]], dtype=torch.long)\n",
    "        tag_ids = torch.tensor([feat_tag2id(tag_feat) for tag_feat in features[5:]],dtype=torch.long)\n",
    "        word_embeds = self.word_embeddings(word_ids).view((1, -1))\n",
    "        tag_embeds = self.tag_embeddings(tag_ids).view((1,-1))\n",
    "        embedding_layer=torch.cat((word_embeds,tag_embeds),1)\n",
    "        out=self.network(embedding_layer)\n",
    "        output=nn.functional.log_softmax(out, dim=1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading training data and writing the training function with Negative Log-Likelihood loss and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(data_path+'.data', 'r').read().strip().split('\\n') \n",
    "minibatch_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epochs,train_data):\n",
    "    model.train()\n",
    "    total_loss=torch.tensor([0.0])\n",
    "    random.shuffle(train_data)\n",
    "    loss_function=nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "    \n",
    "    for epochs in range(epochs):\n",
    "         print('epoch:',epochs+1)\n",
    "        \n",
    "         for j,line in enumerate(train_data):\n",
    "            fields = line.strip().split('\\t')\n",
    "            features, label, gold_label = fields[:-1], fields[-1], tag2id(fields[-1])\n",
    "            result = model(features)\n",
    "            loss = loss_function(result, torch.tensor([gold_label], dtype=torch.long))\n",
    "            total_loss+=loss\n",
    "            if j % minibatch_size == 0:\n",
    "                minibatch_loss = total_loss / minibatch_size\n",
    "                optimizer.zero_grad()\n",
    "                minibatch_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss=torch.tensor([0.0])\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    return result.detach()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= POS_tagging().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model on a fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "train(model,5,train_data)\n",
    "print('finished training!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to classify our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model,ws):\n",
    "   # first putting two start symbols\n",
    "    ws = ['<s>', '<s>'] + ws + ['</s>', '</s>']\n",
    "    ts = ['<s>', '<s>']\n",
    "    with torch.no_grad():\n",
    "        for i in range(2, len(ws) - 2):\n",
    "            features = ws[i - 2:i + 3] + ts[i - 2:i]\n",
    "\n",
    "       # running forward\n",
    "            output = model(features)\n",
    "\n",
    "       # getting best tag\n",
    "            best_tag_id = np.argmax(output)\n",
    "\n",
    "       # assigning the best tag\n",
    "            ts.append(tagid2tag_str(best_tag_id.item()))\n",
    "\n",
    "    return ts[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now upload and classify test data, evaluating the accuracy of the model depending on the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'data/en.pos.dev.raw'\n",
    "writer = open(test_file+'.output.pyex.dim2', 'w')\n",
    "for sentence in open(test_file, 'r'):\n",
    "    words = sentence.strip().split()\n",
    "    tags = decode(model, words)\n",
    "    output = [word + '\\t' + tag for word, tag in zip(words, tags)]\n",
    "    writer.write('\\n'.join(output) + '\\n\\n')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(w_test_file,data_file):\n",
    "    true=0\n",
    "    compare=open(data_file,'r')\n",
    "    l=[]\n",
    "    k=[]\n",
    "    for sentence1 in compare:\n",
    "        words1=sentence1.strip().split()\n",
    "        if len(words1)==2:\n",
    "            l.append(words1[1])\n",
    "    for sentence2 in open(w_test_file,'r'):\n",
    "        words2=sentence2.strip().split()\n",
    "        if len(words2)==2:\n",
    "            k.append(words2[1])\n",
    "    for i in range(len(l)):\n",
    "        if l[i]==k[i]:\n",
    "            true+=1\n",
    "    accuracy=true/len(l)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40361147303323364\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_test('data/en.pos.dev.raw.output.pyex.dim2','data/en.pos.dev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
